import os
import json
import re
import time
import random
import logging
import io
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional
from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, Security, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.security import APIKeyHeader
from pydantic import BaseModel
from google import genai
from google.genai import types, errors
from supabase import create_client, Client
import fitz  # PyMuPDF
from PIL import Image

# ==========================================
# 0. é…ç½®å€åŸŸ & æ—¥èªŒè¨­ç½®
# ==========================================

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

# æ¨¡å‹åç¨±é…ç½®
MODEL_NAME = os.getenv("MODEL_NAME", "gemini-2.5-flash")

# ==========================================
# 1. ç’°å¢ƒè®Šé‡èˆ‡åˆå§‹åŒ–
# ==========================================

def load_env_file():
    """å¾ .env æ–‡ä»¶åŠ è¼‰ç’°å¢ƒè®Šé‡ (å…¼å®¹ UTF-8 å’Œ UTF-16)"""
    env_file = Path(__file__).parent / ".env"
    if not env_file.exists():
        return

    for encoding in ["utf-8", "utf-16"]:
        try:
            with open(env_file, "r", encoding=encoding) as f:
                for line in f:
                    line = line.strip()
                    if line and not line.startswith("#") and "=" in line:
                        key, value = line.split("=", 1)
                        if key.strip() not in os.environ:
                            os.environ[key.strip()] = value.strip('"\'')
            break
        except (UnicodeDecodeError, Exception):
            continue

load_env_file()

# åˆå§‹åŒ– Gemini å®¢æˆ¶ç«¯
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    logger.error("âŒ æœªæ‰¾åˆ° GEMINI_API_KEY")
    raise RuntimeError("GEMINI_API_KEY is missing")

genai_client = genai.Client(api_key=GEMINI_API_KEY)

# ==========================================
# API å¯†é‘°é©—è­‰è¨­ç½®
# ==========================================

# å®šç¾©ä¸€å€‹ç°¡å–®çš„ API Key Header æª¢æŸ¥
API_KEY_NAME = "X-API-SECRET"
api_key_header = APIKeyHeader(name=API_KEY_NAME, auto_error=False)

# å¾ç’°å¢ƒè®Šæ•¸è®€å–ä½ è‡ªå·±è¨­å®šçš„å¯†ç¢¼
INTERNAL_API_SECRET = os.getenv("INTERNAL_API_SECRET", "my_super_secret_password")

async def verify_api_key(api_key: str = Security(api_key_header)):
    """é©—è­‰è«‹æ±‚æ˜¯å¦åŒ…å«æ­£ç¢ºçš„å¯†é‘°"""
    if api_key != INTERNAL_API_SECRET:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Could not validate credentials",
        )

# ==========================================
# 2. Supabase å–®ä¾‹æ¨¡å¼
# ==========================================

class SupabaseManager:
    """ç®¡ç† Supabase é€£æ¥çš„å–®ä¾‹é¡"""
    _instance: Optional[Client] = None
    _key_type: str = "none"

    @classmethod
    def get_client(cls) -> Client:
        if cls._instance:
            return cls._instance

        url = os.getenv("SUPABASE_URL")
        service_key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        anon_key = os.getenv("SUPABASE_KEY")

        if not url:
            logger.warning("âš ï¸ æœªæª¢æ¸¬åˆ° SUPABASE_URL")
            raise RuntimeError("Missing SUPABASE_URL")

        # å„ªå…ˆä½¿ç”¨ Service Role Key ä»¥ç¹é RLS
        key = service_key or anon_key
        if not key:
            raise RuntimeError("Missing SUPABASE_KEY or SUPABASE_SERVICE_ROLE_KEY")

        cls._key_type = "service_role" if service_key else "anon"
        logger.info(f"ğŸ”‘ åˆå§‹åŒ– Supabase Client (Key Type: {cls._key_type})")
        
        cls._instance = create_client(url, key)
        return cls._instance

    @classmethod
    def get_key_type(cls) -> str:
        return cls._key_type

# ==========================================
# 3. FastAPI æ‡‰ç”¨è¨­ç½®
# ==========================================

app = FastAPI(title="AI Feynman Tutor API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "feynman-ai-tutor-amber.vercel.app", # ğŸ”¥ æ–°å¢ Vercel çš„ç¶²å€
        "*" # (æ¸¬è©¦æ™‚å¯å…ˆç”¨ * å…è¨±æ‰€æœ‰ï¼Œä½†ä¸å»ºè­°é•·æœŸä½¿ç”¨)
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# 4. æ•¸æ“šæ¨¡å‹ (Pydantic)
# ==========================================

class GenerateTreeRequest(BaseModel):
    topic: str

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, Any]] = []
    node_context: Dict[str, Any]
    user_id: Optional[str] = None
    tree_id: Optional[str] = None

class SyncNoteRequest(BaseModel):
    user_id: str
    tree_id: str
    node_id: str
    content: str

class ProcessFileRequest(BaseModel):
    user_id: str
    tree_id: str
    node_id: str
    file_path: str  # Supabase Storage ä¸­çš„è·¯å¾‘
    file_type: str  # ä¾‹å¦‚ 'application/pdf'

# ==========================================
# 5. è¼”åŠ©å‡½æ•¸ (Embedding & RAG & Tools)
# ==========================================

def generate_embedding(text: str) -> List[float]:
    """å°‡æ–‡å­—è½‰ç‚º 768 ç¶­å‘é‡ (ä¿®å¾©åƒæ•¸æ ¼å¼å•é¡Œ)"""
    try:
        # ä½¿ç”¨ contents=[text] ç¢ºä¿å›å‚³çµæ§‹ç‚º List
        result = genai_client.models.embed_content(
            model="text-embedding-004",
            contents=[text], 
        )
        
        if hasattr(result, 'embeddings') and result.embeddings:
            return result.embeddings[0].values
        elif hasattr(result, 'embedding') and result.embedding:
            return result.embedding.values
        else:
            logger.error(f"âš ï¸ Embedding format mismatch! Dir: {dir(result)}")
            return []
            
    except Exception as e:
        logger.error(f"âŒ Embedding generation failed: {e}")
        return []

def search_relevant_notes(query: str, user_id: str, node_id: str) -> str:
    """RAG æ ¸å¿ƒï¼šæœç´¢ç›¸é—œç­†è¨˜"""
    try:
        query_vector = generate_embedding(query)
        if not query_vector:
            return ""

        supabase = SupabaseManager.get_client()
        
        # èª¿ç”¨ SQL ä¸­å®šç¾©çš„ RPC å‡½æ•¸
        response = supabase.rpc("match_vectors", {
            "query_embedding": query_vector,
            "match_threshold": 0.3, # é™ä½é–€æª»ä»¥å¢åŠ å¬å›ç‡ (ç­†è¨˜é€šå¸¸è¼ƒçŸ­)
            "match_count": 3,
            "filter_node_id": node_id,
            "filter_user_id": user_id
        }).execute()

        if response.data:
            context_text = "\n".join([f"- {item['content']}" for item in response.data])
            logger.info(f"ğŸ” RAG Hit! Found context: {context_text[:50]}...")
            return context_text
        return ""
    except Exception as e:
        logger.warning(f"âš ï¸ Vector search failed: {e}")
        return ""

def get_flashcard_tool_declaration() -> types.FunctionDeclaration:
    return types.FunctionDeclaration(
        name="create_flashcard_tool",
        description="å‰µå»ºä¸€å¼µé–ƒå¡ã€‚åªæœ‰ç•¶ç”¨æˆ¶æ˜ç¢ºç¸½çµçŸ¥è­˜æˆ–è§£é‡‹æ¦‚å¿µæ™‚æ‰ä½¿ç”¨ã€‚",
        parameters={
            "type": "object",
            "properties": {
                "front": {"type": "string", "description": "é–ƒå¡æ­£é¢å…§å®¹ï¼ˆç¹é«”ä¸­æ–‡ï¼‰"},
                "back": {"type": "string", "description": "é–ƒå¡èƒŒé¢å…§å®¹ï¼ˆç²¾ç°¡ç­”æ¡ˆï¼‰"},
            },
            "required": ["front", "back"],
        },
    )

def execute_create_flashcard(front: str, back: str, user_id: str, tree_id: str, node_id: str) -> Dict[str, Any]:
    """åŸ·è¡Œå¯«å…¥ Supabase çš„é‚è¼¯"""
    logger.info(f"ğŸ› ï¸ Executing Tool: Create Flashcard -> {front}")
    
    if not all([user_id, tree_id, node_id]):
        return {"status": "error", "error": "Missing parameters"}

    try:
        supabase = SupabaseManager.get_client()
        data = {
            "user_id": user_id,
            "tree_id": tree_id,
            "node_id": node_id,
            "front": front,
            "back": back
        }
        
        response = supabase.table("flashcards").insert(data).execute()
        
        if hasattr(response, "data") or (hasattr(response, "status_code") and 200 <= response.status_code < 300):
            logger.info("âœ… Flashcard inserted successfully.")
            return {"status": "success", "front": front}
        else:
            return {"status": "error", "error": "Database insert failed"}

    except Exception as e:
        logger.exception("âŒ Database operation failed")
        return {"status": "error", "error": str(e)}

def download_file_from_supabase(path: str) -> bytes:
    """å¾ node_assets bucket ä¸‹è¼‰æ–‡ä»¶å…§å®¹"""
    try:
        supabase = SupabaseManager.get_client()
        response = supabase.storage.from_("node_assets").download(path)
        logger.info(f"ğŸ“¥ Downloaded file from: {path} ({len(response)} bytes)")
        return response
    except Exception as e:
        logger.error(f"âŒ Failed to download file from {path}: {e}")
        raise HTTPException(status_code=500, detail=f"File download failed: {str(e)}")

def analyze_image_with_gemini(image_bytes: bytes) -> str:
    """ä½¿ç”¨ Gemini 2.5 Flash é€²è¡Œè¦–è¦ºåˆ†æ (OCR + æè¿°)ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶"""
    max_retries = 3
    base_delay = 2  # åŸºç¤ç­‰å¾…ç§’æ•¸

    prompt = "è«‹è©³ç´°è½‰éŒ„é€™å¼µåœ–ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ã€‚å¦‚æœæ˜¯åœ–è¡¨æˆ–åœ–æ¡ˆï¼Œè«‹è©³ç´°æè¿°å…¶ç´°ç¯€å’Œå«ç¾©ã€‚ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸éœ€è¦é–‹å ´ç™½ã€‚"

    for attempt in range(max_retries):
        try:
            # æ§‹å»ºè«‹æ±‚å…§å®¹
            contents = [
                types.Content(
                    role="user",
                    parts=[
                        types.Part.from_text(text=prompt),
                        types.Part.from_bytes(data=image_bytes, mime_type="image/png")
                    ]
                )
            ]

            response = genai_client.models.generate_content(
                model="gemini-1.5-flash",  # é€™è£¡ä½¿ç”¨ 1.5 Flash æ¯”è¼ƒç©©å®š
                contents=contents
            )
            
            if response.text:
                return response.text
                
        except Exception as e:
            # æª¢æŸ¥æ˜¯å¦ç‚º 503 Overloaded
            is_overloaded = "503" in str(e) or "overloaded" in str(e).lower()
            
            if is_overloaded and attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                logger.warning(f"âš ï¸ Vision Model overloaded. Retrying in {delay:.2f}s... (Attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)
                continue # ç¹¼çºŒä¸‹ä¸€æ¬¡è¿´åœˆ
            else:
                # å¦‚æœä¸æ˜¯ 503ï¼Œæˆ–è€…é‡è©¦æ¬¡æ•¸ç”¨ç›¡ï¼Œå‰‡è¨˜éŒ„éŒ¯èª¤ä¸¦æ”¾æ£„
                logger.warning(f"âš ï¸ Vision analysis failed after attempt {attempt + 1}: {e}")
                
    return ""
    """ä½¿ç”¨ Gemini é€²è¡Œè¦–è¦ºåˆ†æ (OCR + æè¿°)ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶"""
    max_retries = 3
    base_delay = 2  # åŸºç¤ç­‰å¾…ç§’æ•¸

    prompt = "è«‹è©³ç´°è½‰éŒ„é€™å¼µåœ–ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ã€‚å¦‚æœæ˜¯åœ–è¡¨æˆ–åœ–æ¡ˆï¼Œè«‹è©³ç´°æè¿°å…¶ç´°ç¯€å’Œå«ç¾©ã€‚ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸éœ€è¦é–‹å ´ç™½ã€‚"

    for attempt in range(max_retries):
        try:
            # æ§‹å»ºè«‹æ±‚å…§å®¹
            contents = [
                types.Content(
                    role="user",
                    parts=[
                        types.Part.from_text(text=prompt),
                        types.Part.from_bytes(data=image_bytes, mime_type="image/png")
                    ]
                )
            ]

            response = genai_client.models.generate_content(
                model="gemini-2.5-flash", 
                contents=contents
            )
            
            if response.text:
                return response.text
                
        except Exception as e:
            # æª¢æŸ¥æ˜¯å¦ç‚º 503 Overloaded
            is_overloaded = "503" in str(e) or "overloaded" in str(e).lower()
            
            if is_overloaded and attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                logger.warning(f"âš ï¸ Vision Model overloaded. Retrying in {delay:.2f}s... (Attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)
                continue # ç¹¼çºŒä¸‹ä¸€æ¬¡è¿´åœˆ
            else:
                # å¦‚æœä¸æ˜¯ 503ï¼Œæˆ–è€…é‡è©¦æ¬¡æ•¸ç”¨ç›¡ï¼Œå‰‡è¨˜éŒ„éŒ¯èª¤ä¸¦æ”¾æ£„
                logger.warning(f"âš ï¸ Vision analysis failed after attempt {attempt + 1}: {e}")
                
    return ""
    """ä½¿ç”¨ Gemini 2.5 Flash é€²è¡Œè¦–è¦ºåˆ†æ (OCR + æè¿°)"""
    try:
        # æ§‹å»º Prompt
        prompt = "è«‹è©³ç´°è½‰éŒ„é€™å¼µåœ–ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—ã€‚å¦‚æœæ˜¯åœ–è¡¨æˆ–åœ–æ¡ˆï¼Œè«‹è©³ç´°æè¿°å…¶ç´°ç¯€å’Œå«ç¾©ã€‚ç›´æ¥è¼¸å‡ºå…§å®¹ï¼Œä¸éœ€è¦é–‹å ´ç™½ã€‚"
        
        # ğŸ”¥ ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¢ºçš„ SDK çµæ§‹å‚³éåœ–ç‰‡èˆ‡æ–‡å­—
        # Google Gen AI SDK v1.0+ å¯«æ³•
        response = genai_client.models.generate_content(
            model="gemini-2.5-flash",  # ğŸ”¥ éµç…§æŒ‡ä»¤ï¼šä½¿ç”¨ 2.5
            contents=[
                types.Content(
                    role="user",
                    parts=[
                        types.Part.from_text(text=prompt),
                        types.Part.from_bytes(data=image_bytes, mime_type="image/png")
                    ]
                )
            ]
        )
        
        if response.text:
            return response.text
        return ""
        
    except Exception as e:
        logger.warning(f"âš ï¸ Vision analysis failed: {e}")
        return ""
def extract_text_from_pdf(file_content: bytes) -> str:
    """ä½¿ç”¨æ··åˆç­–ç•¥è§£æ PDFï¼šæ–‡å­—æå– + Vision OCR"""
    try:
        doc = fitz.open(stream=file_content, filetype="pdf")
        text = ""
        
        for page_num in range(doc.page_count):
            page = doc[page_num]
            
            # å…ˆå˜—è©¦ç›´æ¥æå–æ–‡å­—
            page_text = page.get_text()
            
            # åˆ¤æ–·æ˜¯å¦ç‚ºæƒææª”æˆ–ç´”åœ–ç‰‡ï¼ˆæ–‡å­—é•·åº¦å°æ–¼50å­—ï¼‰
            if len(page_text.strip()) < 50:
                logger.info(f"ğŸ“· Page {page_num + 1} appears to be scanned/image, using Vision API...")
                
                try:
                    # å°‡é é¢è½‰æ›ç‚ºåœ–ç‰‡
                    pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x è§£æåº¦æå‡å“è³ª
                    img_bytes = pix.tobytes("png")
                    
                    # ä½¿ç”¨ Vision API åˆ†æ
                    vision_text = analyze_image_with_gemini(img_bytes)
                    
                    if vision_text:
                        text += f"\n--- ç¬¬ {page_num + 1} é  (Vision OCR) ---\n{vision_text}\n"
                        logger.info(f"âœ… Page {page_num + 1}: Vision OCR successful ({len(vision_text)} chars)")
                    else:
                        text += f"\n--- ç¬¬ {page_num + 1} é  (ç„¡æ³•è­˜åˆ¥) ---\n"
                        logger.warning(f"âš ï¸ Page {page_num + 1}: Vision OCR failed")
                        
                except Exception as vision_error:
                    logger.warning(f"âš ï¸ Page {page_num + 1} vision processing failed: {vision_error}")
                    text += f"\n--- ç¬¬ {page_num + 1} é  (è™•ç†å¤±æ•—) ---\n"
            else:
                # æ–‡å­—è¶³å¤ ï¼Œç›´æ¥ä½¿ç”¨æå–çš„æ–‡å­—
                text += f"\n--- ç¬¬ {page_num + 1} é  ---\n{page_text}\n"
                logger.info(f"ğŸ“ Page {page_num + 1}: Direct text extraction ({len(page_text)} chars)")
        
        doc.close()
        logger.info(f"ğŸ“„ PDF processing completed: {len(text)} total characters")
        return text
        
    except Exception as e:
        logger.error(f"âŒ Failed to extract text from PDF: {e}")
        raise HTTPException(status_code=500, detail=f"PDF text extraction failed: {str(e)}")

def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[str]:
    """å°‡é•·æ–‡æœ¬åˆ‡åˆ†æˆå¤šå€‹å°ç‰‡æ®µï¼Œä¿æŒä¸€å®šçš„é‡ç–Šä»¥ä¿ç•™ä¸Šä¸‹æ–‡"""
    if not text.strip():
        return []
    
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        end = start + chunk_size
        
        # å¦‚æœä¸æ˜¯æœ€å¾Œä¸€å€‹chunkï¼Œå˜—è©¦åœ¨å¥è™Ÿæˆ–æ›è¡Œç¬¦è™•åˆ‡åˆ†
        if end < text_length:
            # å°‹æ‰¾æœ€è¿‘çš„å¥è™Ÿæˆ–æ›è¡Œç¬¦
            for i in range(end, max(start + chunk_size // 2, end - 200), -1):
                if text[i] in '.ã€‚\n':
                    end = i + 1
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # ä¸‹ä¸€å€‹chunkçš„èµ·å§‹ä½ç½®è€ƒæ…®é‡ç–Š
        start = max(start + 1, end - overlap)
        
        # é¿å…ç„¡é™å¾ªç’°
        if start >= text_length:
            break
    
    logger.info(f"ğŸ“ Text chunked into {len(chunks)} pieces (chunk_size={chunk_size}, overlap={overlap})")
    return chunks

# ==========================================
# 6. API è·¯ç”±
# ==========================================

@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥"""
    status = {"status": "ok", "supabase": False}
    try:
        supabase = SupabaseManager.get_client()
        supabase.table("flashcards").select("id").limit(1).execute()
        status["supabase"] = True
    except Exception as e:
        status["status"] = "warning"
        status["error"] = str(e)
    return status

@app.post("/sync-note", dependencies=[Depends(verify_api_key)])
async def sync_note_vector(request: SyncNoteRequest):
    """ç•¶ç”¨æˆ¶ä¿å­˜ç­†è¨˜æ™‚ï¼Œæ›´æ–°å‘é‡åº«"""
    try:
        logger.info(f"ğŸ“ Syncing note for node: {request.node_id}")
        vector = generate_embedding(request.content)
        if not vector:
            raise HTTPException(status_code=500, detail="Failed to generate embedding")

        supabase = SupabaseManager.get_client()
        
        # åˆªé™¤èˆŠå‘é‡
        supabase.table("node_vectors").delete().eq("node_id", request.node_id).eq("source_type", "note").execute()
        logger.info(f"ğŸ—‘ï¸ Deleted old note vectors for node: {request.node_id}")
        
        # æ’å…¥æ–°å‘é‡
        data = {
            "user_id": request.user_id,
            "tree_id": request.tree_id,
            "node_id": request.node_id,
            "content": request.content,
            "source_type": "note",
            "embedding": vector
        }
        supabase.table("node_vectors").insert(data).execute()
        logger.info("âœ… Note vector inserted successfully")
        
        return {"status": "success"}
    except Exception as e:
        logger.exception("Sync note failed")
        return JSONResponse(status_code=500, content={"message": str(e)})

@app.post("/generate-tree", dependencies=[Depends(verify_api_key)])
async def generate_tree(request: GenerateTreeRequest):
    """ç”ŸæˆçŸ¥è­˜æ¨¹ (åŒ…å« 503 é‡è©¦æ©Ÿåˆ¶)"""
    logger.info(f"ğŸŒ³ Generating tree for: {request.topic}")
    
    max_retries = 3
    base_delay = 1

    prompt = f"""
### Role
ä½ æ˜¯ä¸€ä½ç²¾é€šå„é ˜åŸŸçŸ¥è­˜çµæ§‹çš„**è³‡æ·±èª²ç¨‹è¨­è¨ˆå¸«èˆ‡çŸ¥è­˜åœ–è­œå°ˆå®¶**ã€‚ä½ æ“…é•·å°‡è¤‡é›œçš„ä¸»é¡Œæ‹†è§£ç‚ºçµæ§‹åŒ–çš„å­¸ç¿’è·¯å¾‘ã€‚

### Task
è«‹ç‚ºä¸»é¡Œ "{request.topic}" ç”Ÿæˆä¸€å€‹**çµæ§‹åŒ–çš„å­¸ç¿’çŸ¥è­˜æ¨¹**ã€‚

### Requirements
1. **å‹•æ…‹å±¤ç´šçµæ§‹ (Adaptive Hierarchy)**ï¼š
   - æ¨¹çš„æ·±åº¦èˆ‡å»£åº¦æ‡‰å–æ±ºæ–¼ä¸»é¡Œ "{request.topic}" çš„å®è§€ç¨‹åº¦ã€‚
   - è‹¥ä¸»é¡Œå®è§€ï¼ˆå¦‚ "Computer Science"ï¼‰ï¼Œçµæ§‹æ‡‰æ·±å±¤ä¸”è¤‡é›œï¼ˆRoot -> é ˜åŸŸ -> å­é ˜åŸŸ -> æ ¸å¿ƒæ¦‚å¿µ -> çŸ¥è­˜é»ï¼‰ã€‚
   - è‹¥ä¸»é¡Œå…·é«”ï¼ˆå¦‚ "Python List"ï¼‰ï¼Œçµæ§‹æ‡‰è¼ƒæ·ºï¼Œå°ˆæ³¨æ–¼ç´°ç¯€æ‹†è§£ã€‚
   
2. **åŸå­åŒ–çŸ¥è­˜é» (Atomic Leaf Nodes)**ï¼š
   - æ¨¹ç‹€çµæ§‹çš„æœ€åº•å±¤ï¼ˆè‘‰ç¯€é»ï¼‰å¿…é ˆæ˜¯ã€ŒåŸå­åŒ–çŸ¥è­˜é»ã€ã€‚
   - å®šç¾©ï¼š**ç„¡æ³•å†æœ‰æ„ç¾©åœ°ç´°åˆ†**çš„å–®ä¸€æ¦‚å¿µæˆ–æŠ€èƒ½ï¼ˆä¾‹å¦‚ï¼šã€Œè®Šæ•¸å‘½åè¦å‰‡ã€æ˜¯åŸå­é»ï¼Œã€ŒPython åŸºç¤ã€å‰‡ä¸æ˜¯ï¼Œå› ç‚ºå®ƒé‚„å¯ä»¥ç´°åˆ†ï¼‰ã€‚

3. **èªè¨€è‡ªé©æ‡‰ (Language Matching)**ï¼š
   - ç¯€é»çš„ `label` å’Œ `description` èªè¨€å¿…é ˆèˆ‡è¼¸å…¥ä¸»é¡Œ "{request.topic}" çš„èªè¨€åš´æ ¼ä¿æŒä¸€è‡´ã€‚
   - è‹¥è¼¸å…¥æ˜¯è‹±æ–‡ï¼Œå‰‡å…¨è‹±æ–‡è¼¸å‡ºï¼›è‹¥è¼¸å…¥æ˜¯ç¹é«”ä¸­æ–‡ï¼Œå‰‡å…¨ç¹é«”ä¸­æ–‡è¼¸å‡ºã€‚

4. **æ•¸æ“šçµæ§‹ (Adjacency List)**ï¼š
   - é›–ç„¶æ˜¯æ¨¹ç‹€é‚è¼¯ï¼Œä½†è«‹è¿”å›å¸¶æœ‰ `parentId` çš„æ‰å¹³åŒ–åˆ—è¡¨ï¼ˆFlat Listï¼‰ã€‚
   - æ ¹ç¯€é»çš„ `parentId` ç‚º `null`ã€‚

### Output Format
è«‹åƒ…è¿”å›ä¸€å€‹ç´” JSON å°è±¡ï¼Œä¸è¦åŒ…å«ä»»ä½• Markdown æ¨™è¨˜æˆ–é¡å¤–æ–‡å­—ï¼š
{{
  "nodes": [
    {{
      "id": "å”¯ä¸€æ¨™è­˜ç¬¦ (string)",
      "label": "ç¯€é»åç¨± (string)",
      "description": "ç°¡çŸ­çš„å­¸ç¿’ç›®æ¨™æˆ–å®šç¾© (string)",
      "parentId": "çˆ¶ç¯€é»ID (string, root ç‚º null)"
    }}
    ...
  ]
}}
"""

    for attempt in range(max_retries):
        try:
            response = genai_client.models.generate_content(
                model=MODEL_NAME,
                contents=prompt,
                config=types.GenerateContentConfig(
                    temperature=0.3,
                    response_mime_type="application/json"
                )
            )
            
            content = re.sub(r'```json\s*|```\s*$', '', response.text).strip()
            parsed = json.loads(content)
            nodes = parsed.get("nodes", parsed)
            return {"nodes": nodes}

        except errors.ServerError as e:
            if e.code == 503:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                logger.warning(f"âš ï¸ Model overloaded (503). Retrying in {delay:.2f}s... (Attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)
            else:
                raise e
        except Exception as e:
            logger.exception("Tree generation failed")
            raise HTTPException(status_code=500, detail=str(e))
    
    raise HTTPException(status_code=503, detail="Service unavailable after max retries")

@app.post("/process-file", dependencies=[Depends(verify_api_key)])
async def process_file(request: ProcessFileRequest):
    """è™•ç†æ–‡ä»¶ï¼šä¸‹è¼‰ã€è§£æã€åˆ‡å¡Šä¸¦å­˜å…¥å‘é‡åº«"""
    try:
        logger.info(f"ğŸ“ Processing file: {request.file_path} for node: {request.node_id}")
        
        # 1. ä¸‹è¼‰æ–‡ä»¶
        file_content = download_file_from_supabase(request.file_path)
        
        # 2. æ ¹æ“šæ–‡ä»¶é¡å‹è§£ææ–‡å­—
        if request.file_type == "application/pdf":
            text = extract_text_from_pdf(file_content)
        else:
            raise HTTPException(status_code=400, detail=f"Unsupported file type: {request.file_type}")
        
        if not text.strip():
            raise HTTPException(status_code=400, detail="No text content found in file")
        
        # 3. åˆ‡å¡Š
        chunks = chunk_text(text)
        if not chunks:
            raise HTTPException(status_code=400, detail="Failed to create text chunks")
        
        # 4. æ¸…ç†èˆŠçš„æ–‡ä»¶å‘é‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        supabase = SupabaseManager.get_client()
        supabase.table("node_vectors").delete().eq("node_id", request.node_id).eq("source_type", "file").execute()
        logger.info(f"ğŸ—‘ï¸ Deleted old file vectors for node: {request.node_id}")
        
        # 5. æ‰¹æ¬¡è™•ç†å‘é‡
        processed_chunks = 0
        batch_data = []
        
        for i, chunk in enumerate(chunks):
            try:
                # ç”Ÿæˆå‘é‡
                vector = generate_embedding(chunk)
                if not vector:
                    logger.warning(f"âš ï¸ Failed to generate embedding for chunk {i+1}")
                    continue
                
                # æº–å‚™æ•¸æ“š
                data = {
                    "user_id": request.user_id,
                    "tree_id": request.tree_id,
                    "node_id": request.node_id,
                    "content": chunk,
                    "source_type": "file",
                    "embedding": vector,
                    "metadata": {"file_path": request.file_path, "chunk_index": i}
                }
                batch_data.append(data)
                processed_chunks += 1
                
                # æ¯10å€‹chunkæ‰¹æ¬¡æ’å…¥ä¸¦ä¼‘æ¯
                if len(batch_data) >= 10:
                    supabase.table("node_vectors").insert(batch_data).execute()
                    logger.info(f"âœ… Inserted batch of {len(batch_data)} chunks")
                    batch_data = []
                    time.sleep(1)  # é¿å… API Rate Limit
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to process chunk {i+1}: {e}")
                continue
        
        # æ’å…¥å‰©é¤˜çš„chunks
        if batch_data:
            supabase.table("node_vectors").insert(batch_data).execute()
            logger.info(f"âœ… Inserted final batch of {len(batch_data)} chunks")
        
        logger.info(f"ğŸ‰ File processing completed: {processed_chunks} chunks processed")
        return {
            "status": "success",
            "chunks_processed": processed_chunks,
            "total_chunks": len(chunks),
            "file_path": request.file_path
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("File processing failed")
        return JSONResponse(status_code=500, content={"message": str(e)})

@app.post("/chat", dependencies=[Depends(verify_api_key)])
async def chat_endpoint(request: ChatRequest):
    """èŠå¤©æ¥å£ - é©é… Gemini 2.0/2.5 æ€è€ƒæ¨¡å‹ + RAG åŠŸèƒ½ (åŒ…å«é‡è©¦æ©Ÿåˆ¶)"""
    max_retries = 3
    base_delay = 1
    
    for attempt in range(max_retries):
        try:
            # 1. RAG æª¢ç´¢ç›¸é—œç­†è¨˜
            rag_context = ""
            if request.user_id and request.node_context.get("id"):
                rag_context = search_relevant_notes(
                    query=request.message,
                    user_id=request.user_id,
                    node_id=request.node_context.get("id")
                )
            
            # 2. æ§‹å»º System Prompt (æ³¨å…¥ RAG å…§å®¹)
            system_instruction = (
                f"ä½ ç¾åœ¨ä¸æ˜¯ä¸€å€‹æ™®é€šçš„èŠå¤©æ©Ÿå™¨äººï¼Œä½ æ˜¯ã€ŒçŸ¥è­˜æ•ç²ç³»çµ±ã€ã€‚\n"
                f"ç•¶å‰ä¸Šä¸‹æ–‡ï¼š{request.node_context.get('label', 'æœªçŸ¥ç¯€é»')} - {request.node_context.get('description', '')}\n"
            )
            
            # æ³¨å…¥ RAG å…§å®¹
            if rag_context.strip():
                system_instruction += (
                    f"\nã€åƒè€ƒç­†è¨˜ã€‘ï¼šç”¨æˆ¶ä¹‹å‰åœ¨é€™å€‹ç¯€é»å¯«éä»¥ä¸‹ç­†è¨˜ï¼Œè«‹åƒè€ƒé€™äº›å…§å®¹ä¾†è¼”åŠ©å›ç­”ï¼š\n{rag_context}\n\n"
                )
            
            system_instruction += (
                "ä½ çš„è¡Œç‚ºæº–å‰‡ï¼š\n"
                "1. **å„ªå…ˆå›ç­”å•é¡Œ**ï¼šå¦‚æœç”¨æˆ¶æ˜¯åœ¨æå•ï¼ˆä¾‹å¦‚ã€Œæˆ‘ç­†è¨˜å¯«äº†ä»€éº¼ï¼Ÿã€ã€Œè§£é‡‹ä¸€ä¸‹é€™å€‹æ¦‚å¿µã€ï¼‰ï¼Œè«‹æ ¹æ“šã€åƒè€ƒç­†è¨˜ã€‘æˆ–ä½ çš„çŸ¥è­˜åº«ç›´æ¥å›ç­”ï¼Œ**ä¸è¦**èª¿ç”¨å·¥å…·ã€‚\n"
                "2. **æ•æ‰å­¸ç¿’æˆæœ**ï¼šåªæœ‰ç•¶ç”¨æˆ¶æ˜ç¢ºåœ°**åšå‡ºç¸½çµ**ã€**è§£é‡‹æ¦‚å¿µ**ã€æˆ–**èªªã€Œæˆ‘æ‡‚äº†ï¼Œæ˜¯...ã€**æ™‚ï¼Œæ‰è¦–ç‚ºã€Œæ•ç²æ™‚åˆ»ã€ï¼Œé€™æ™‚å¿…é ˆèª¿ç”¨ `create_flashcard_tool`ã€‚\n"
                "3. âš ï¸ **ç¦æ­¢**åœ¨ç”¨æˆ¶æå•æ™‚å»ºç«‹é–ƒå¡ã€‚ä¾‹å¦‚ç”¨æˆ¶å•ã€Œä»€éº¼æ˜¯ masuxingï¼Ÿã€ï¼Œä½ æ‡‰è©²å›ç­”å®ƒï¼Œè€Œä¸æ˜¯æŠŠå®ƒåšæˆå¡ç‰‡ã€‚\n"
                "4. å¦‚æœç”¨æˆ¶é‚„æ²’è½æ‡‚ï¼Œå°±ç¹¼çºŒç”¨è˜‡æ ¼æ‹‰åº•æ–¹å¼å¼•å°ï¼Œä¸è¦èª¿ç”¨å·¥å…·ã€‚\n"
            )

            # 3. æ­·å²è¨Šæ¯è½‰æ›
            gemini_history = []
            for msg in request.history:
                role = "model" if msg["role"] in ["assistant", "model"] else "user"
                gemini_history.append(
                    types.Content(role=role, parts=[types.Part.from_text(text=str(msg["content"]))])
                )

            logger.info(f"ğŸ¤– Sending request to {MODEL_NAME}...")

            # 4. æ³¨å…¥å¼·åˆ¶æŒ‡ä»¤ (Prompt Injection)
            user_message_with_instruction = (
                f"{request.message}\n\n"
                "ã€ç³»çµ±ç›£æ§ã€‘ï¼šè«‹åˆ¤æ–·ç”¨æˆ¶æ„åœ–ã€‚\n"
                "- å¦‚æœä»–åœ¨**æå•**æˆ–**ç´¢å–è³‡è¨Š** -> è«‹ç›´æ¥å›ç­”ï¼ˆä¸è¦å»ºå¡ï¼‰ã€‚\n"
                "- å¦‚æœä»–åœ¨**è¼¸å‡ºçŸ¥è­˜**æˆ–**ç¸½çµ** -> è«‹ç«‹åˆ»èª¿ç”¨ `create_flashcard_tool`ã€‚"
            )

            contents = gemini_history + [
                types.Content(role="user", parts=[types.Part.from_text(text=user_message_with_instruction)])
            ]

            # 5. èª¿ç”¨ Gemini
            tools = [types.Tool(function_declarations=[get_flashcard_tool_declaration()])]
            response = genai_client.models.generate_content(
                model=MODEL_NAME,
                contents=contents,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    tools=tools,
                    temperature=0.3,
                )
            )

            # =================================================
            # ğŸ”¥ Gemini 2.0 å›è¦†è§£æé‚è¼¯ (éæ¿¾ Thinking) ğŸ”¥
            # =================================================
            final_text = ""
            function_called = False
            
            if response.candidates and response.candidates[0].content.parts:
                for part in response.candidates[0].content.parts:
                    
                    # A. è™•ç†å·¥å…·èª¿ç”¨ (Function Call)
                    if part.function_call:
                        fn_name = part.function_call.name
                        logger.info(f"âš¡ Function call detected: {fn_name}")
                        
                        if fn_name == "create_flashcard_tool":
                            args = part.function_call.args
                            # å…¼å®¹ args ç‚º dict æˆ– object çš„æƒ…æ³
                            front = args.get("front") if isinstance(args, dict) else getattr(args, "front", "")
                            back = args.get("back") if isinstance(args, dict) else getattr(args, "back", "")
                            
                            result = execute_create_flashcard(
                                front=front, back=back,
                                user_id=request.user_id,
                                tree_id=request.tree_id,
                                node_id=request.node_context.get("id")
                            )
                            
                            function_called = True
                            if result["status"] == "success":
                                final_text += f"\n\n(âœ¨ ç³»çµ±æç¤ºï¼šå·²ç‚ºæ‚¨ç”Ÿæˆé–ƒå¡ï¼æ­£é¢ï¼š{front})"
                            else:
                                final_text += f"\n\n(âš ï¸ ç³»çµ±æç¤ºï¼šé–ƒå¡å‰µå»ºå¤±æ•— - {result.get('error')})"

                    # B. è™•ç†æ™®é€šæ–‡æœ¬ (Text) - éæ¿¾æ‰ Thought
                    elif hasattr(part, "text") and part.text:
                        final_text += part.text
                    
                    # C. å¿½ç•¥ Thought é¡å‹

            # 6. å…œåº•å›æ‡‰
            if not final_text.strip():
                if function_called:
                    final_text = "é‡é»æˆ‘å¹«ä½ è¨˜ä¸‹ä¾†äº†ï¼(âœ¨ ç³»çµ±ç”Ÿæˆé–ƒå¡)"
                else:
                    logger.warning("âš ï¸ Model returned empty response without tool call.")
                    final_text = "ï¼ˆAI ä¼¼ä¹æ­£åœ¨æ·±åº¦æ€è€ƒï¼Œè«‹è©¦è‘—ç¹¼çºŒä½ çš„æ€è·¯...ï¼‰"

            return {"reply": final_text}

        except errors.ServerError as e:
            if e.code == 503 and attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                logger.warning(f"âš ï¸ Chat Model overloaded (503). Retrying in {delay:.2f}s... (Attempt {attempt + 1}/{max_retries})")
                time.sleep(delay)
                continue
            else:
                logger.exception("Chat endpoint ServerError")
                return JSONResponse(status_code=503, content={"message": "AI æœå‹™æš«æ™‚éè¼‰ï¼Œè«‹ç¨å¾Œå†è©¦"})
        except Exception as e:
            logger.exception("Chat endpoint error")
            return JSONResponse(status_code=500, content={"message": str(e)})
    
    # å¦‚æœæ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†
    return JSONResponse(status_code=503, content={"message": "AI æœå‹™æš«æ™‚ä¸å¯ç”¨ï¼Œè«‹ç¨å¾Œå†è©¦"})

@app.post("/transcribe-audio", dependencies=[Depends(verify_api_key)])
async def transcribe_audio(file: UploadFile = File(...)):
    """èªéŸ³è½‰æ–‡å­—ç«¯é» - æ”¯æ´è‹±æ–‡ã€ä¸­æ–‡å’Œç²µèªï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶"""
    max_retries = 3
    base_delay = 2  # åŸºç¤ç­‰å¾…ç§’æ•¸
    
    try:
        logger.info(f"ğŸ™ï¸ Processing audio file: {file.filename} ({file.content_type})")
        
        # è®€å–éŸ³é »æ–‡ä»¶çš„ bytes
        audio_bytes = await file.read()
        
        if not audio_bytes:
            raise HTTPException(status_code=400, detail="Empty audio file")
        
        # æ§‹å»º Prompt
        prompt = "è«‹é€å­—è½‰éŒ„é€™æ®µèªéŸ³ã€‚èªéŸ³å¯èƒ½æ˜¯è‹±æ–‡ã€ä¸­æ–‡æˆ–ç²µèªï¼ˆå»£æ±è©±ï¼‰ã€‚è«‹å¿½ç•¥èªæ°£è©ï¼Œç›´æ¥è¼¸å‡ºè½‰éŒ„å¾Œçš„ç´”æ–‡å­—ï¼Œä¸è¦åŠ ä»»ä½•é–‹å ´ç™½ã€‚"
        
        # ä½¿ç”¨ Gemini 1.5 Flash é€²è¡ŒèªéŸ³è½‰éŒ„ï¼ŒåŒ…å«é‡è©¦æ©Ÿåˆ¶
        for attempt in range(max_retries):
            try:
                contents = [
                    types.Content(
                        role="user",
                        parts=[
                            types.Part.from_text(text=prompt),
                            types.Part.from_bytes(data=audio_bytes, mime_type=file.content_type)
                        ]
                    )
                ]
                
                response = genai_client.models.generate_content(
                    model="gemini-2.0-flash",  # ä½¿ç”¨ 1.5 Flashï¼Œè™•ç†éŸ³é »ç©©å®šä¸”ä¾¿å®œ
                    contents=contents
                )
                
                if response.text:
                    transcribed_text = response.text.strip()
                    logger.info(f"âœ… Audio transcription successful: {transcribed_text[:50]}...")
                    return {"text": transcribed_text}
                else:
                    logger.warning("âš ï¸ Gemini returned empty transcription")
                    if attempt == max_retries - 1:
                        raise HTTPException(status_code=500, detail="Transcription failed - empty response")
                    continue
                    
            except errors.ServerError as e:
                if e.code == 503:  # Model overloaded
                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
                    logger.warning(f"âš ï¸ Model overloaded (503). Retrying in {delay:.1f}s... (attempt {attempt + 1}/{max_retries})")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(delay)
                        continue
                    else:
                        raise HTTPException(
                            status_code=503, 
                            detail="èªéŸ³è½‰éŒ„æœå‹™æš«æ™‚éè¼‰ï¼Œè«‹ç¨å¾Œå†è©¦"
                        )
                else:
                    logger.error(f"âŒ Server error during transcription: {e}")
                    raise HTTPException(status_code=500, detail=f"è½‰éŒ„æœå‹™éŒ¯èª¤: {str(e)}")
                    
            except errors.ClientError as e:
                logger.error(f"âŒ Client error during transcription: {e}")
                raise HTTPException(status_code=400, detail=f"éŸ³é »æ ¼å¼éŒ¯èª¤: {str(e)}")
                
            except Exception as e:
                logger.error(f"âŒ Unexpected error during transcription attempt {attempt + 1}: {e}")
                if attempt == max_retries - 1:
                    raise HTTPException(status_code=500, detail=f"è½‰éŒ„å¤±æ•—: {str(e)}")
                
                # å°æ–¼å…¶ä»–éŒ¯èª¤ä¹Ÿé€²è¡Œé‡è©¦ï¼Œä½†å»¶é²è¼ƒçŸ­
                delay = 1 * (attempt + 1)
                await asyncio.sleep(delay)
                continue
            
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Audio transcription failed")
        raise HTTPException(status_code=500, detail=f"Transcription error: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)